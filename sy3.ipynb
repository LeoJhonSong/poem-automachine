{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "# from torchnet import meter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    '''读入数据'''\n",
    "    datas = np.load(\"tang.npz\", allow_pickle=True)\n",
    "    data = datas['data']\n",
    "    ix2word = datas['ix2word'].item()  # index to word\n",
    "    word2ix = datas['word2ix'].item()\n",
    "\n",
    "    data = torch.from_numpy(data)  # 转为torch.Tensor\n",
    "    return ix2word, word2ix, data\n",
    "\n",
    "\n",
    "def peek_data(row, data, ix2word):\n",
    "    '''查看row行开始的5行数据'''\n",
    "    for i in range(row, row + 5):\n",
    "        print(\"\".join([ix2word[int(j)] for j in data[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix2word, word2ix, data = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57580, 125])\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><START>度门能不访，冒雪屡西东。已想人如玉，遥怜马似骢。乍迷金谷路，稍变上阳宫。还比相思意，纷纷正满空。<EOP>\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><START>逍遥东城隅，双树寒葱蒨。广庭流华月，高阁凝余霰。杜门非养素，抱疾阻良䜩。孰谓无他人，思君岁云变。官曹亮先忝，陈躅慙俊彥。岂知晨与夜，相代不相见。缄书问所如，詶藻当芬绚。<EOP>\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><START>川上风雨来，须臾满城阙。岧峣青莲界，萧条孤兴发。前山遽已净，阴霭夜来歇。乔木生夏凉，流云吐华月。严城自有限，一水非难越。相望曙河远，高斋坐超忽。<EOP>\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><START>庭树忽已暗，故人那不来。祗因厌烦暑，永日坐霜台。<EOP>\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><START>官荣多所系，闲居亦愆期。高阁犹相望，青山欲暮时。<EOP>\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "peek_data(0, data, ix2word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据清洗\n",
    "\n",
    "从上面数据片段能看出其中包含大量空格, 需要去除以排除对正确率的干扰.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    '''将数据平整为一维并滤除空格数据'''\n",
    "    data = data.view(-1)\n",
    "    data = data[data != word2ix['</s>']]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START>度门能不访，冒雪屡西东。已想人如玉，遥怜马似骢。乍迷金谷路，稍变上阳宫。还比相思意，纷纷正满空。<EOP><START>逍遥东城隅，双树寒葱蒨。广庭流华月，高阁凝余霰。杜门非养素\n"
     ]
    }
   ],
   "source": [
    "print(''.join([ix2word[int(j)] for j in prepare_data(data)[:80]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader 构造\n",
    "\n",
    "然后将数据集划分为训练集和验证集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryDataSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.seq_len = 48  # 8首五言/6首七言为一批\n",
    "        self.data = prepare_data(data).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        '''数据集样本批数'''\n",
    "        return len(self.data) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data[index * self.seq_len: (index + 1) * self.seq_len]\n",
    "        # 每个字的标签为下一个字, 配合LSTM如此才能用上一句每个字给出有变化的下一句, 是字符级语言模型\n",
    "        label = self.data[index * self.seq_len + 1: (index + 1) * self.seq_len + 1]  # 这里其实可能越界\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 拆分为训练集和测试集\n",
    "train_data = data[:, :]\n",
    "dataset = PoetryDataSet(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构造\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_LAYERS = 3  # 模型LSTM层数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(PoetryModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=LSTM_LAYERS, batch_first=True)\n",
    "        # self.linear = nn.Linear(self.hidden_dim, vocab_size)\n",
    "        self.fc1 = nn.Linear(self.hidden_dim, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 4096)\n",
    "        self.fc3 = nn.Linear(4096, vocab_size)  # 将输出转为词表的维度\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        batch_size, seq_len = input.size()\n",
    "        embeds = self.embeddings(input)  # 将汉字转为embedding向量\n",
    "        # FIXME: hidden是给定的输入句子, 本来不需要, 但如果给定了几个首句, 就需要这个hidden然后再开始预测\n",
    "        if hidden is None:\n",
    "            # h, c为LSTM的hidden的隐状态和细胞状态, 一般初始化为0\n",
    "            h_0 = input.data.new(LSTM_LAYERS, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            c_0 = input.data.new(LSTM_LAYERS, batch_size, self.hidden_dim).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "        output, hidden = self.lstm(embeds, (h_0, c_0))\n",
    "        # output = self.linear(output)\n",
    "        # 改用3层全连接层处理来自LSTM的输出\n",
    "        # 改用tanh作为全连接层的激活函数, <tanh效果比relu好?>\n",
    "        output = torch.tanh(self.fc1(output))\n",
    "        output = torch.tanh(self.fc2(output))\n",
    "        output = self.fc3(output)\n",
    "        output = output.reshape(batch_size * seq_len, -1)\n",
    "        return output, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs, batch_size, device, lr, scheduler_kwargs, criterion, tensorboard_path):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)  # 使用dataloader多进程分批加载数据\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # 可学习参数, 学习率 (超参数)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, **scheduler_kwargs)  # 学习率调整\n",
    "    # loss_meter = meter.AverageValueMeter() # torchnet.meter.AverageValueMeter\n",
    "\n",
    "    model.train() # 设置模型为训练模式\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for batch, (text, label) in enumerate(dataloader):\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            # 正向传播\n",
    "            output, _ = model(text)\n",
    "            # 计算损失\n",
    "            loss = criterion(output, label.reshape(-1))  # output平整为了一维所以label也要平整来对齐\n",
    "            # 置零所有参数的梯度\n",
    "            optimizer.zero_grad()\n",
    "            # 反向传播计算梯度\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            # loss_meter.add(loss.item())\n",
    "            if batch % 500 == 0:\n",
    "                print(f'\\tepoch: {epoch}, batch: {batch}, loss: {loss.item()}')\n",
    "        scheduler.step() # 更新学习率\n",
    "        print(f'epoch: {epoch}, average loss: {train_loss / len(dataloader)}')\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 1024\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "print(device := torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss: 9.022175788879395\n",
      "epoch: 0, batch: 100, loss: 6.049218654632568\n",
      "epoch: 0, batch: 200, loss: 6.101924419403076\n",
      "epoch: 0, batch: 300, loss: 5.931379795074463\n",
      "epoch: 0, batch: 400, loss: 5.891652584075928\n",
      "epoch: 0, batch: 500, loss: 5.73443078994751\n",
      "epoch: 0, batch: 600, loss: 5.71327543258667\n",
      "epoch: 0, batch: 700, loss: 5.548266887664795\n",
      "epoch: 0, batch: 800, loss: 5.467962265014648\n",
      "epoch: 0, batch: 900, loss: 5.334251403808594\n",
      "epoch: 0, batch: 1000, loss: 5.3406453132629395\n",
      "epoch: 0, batch: 1100, loss: 5.236422061920166\n",
      "epoch: 0, batch: 1200, loss: 5.4016242027282715\n",
      "epoch: 0, batch: 1300, loss: 5.1095733642578125\n",
      "epoch: 0, batch: 1400, loss: 5.317927837371826\n",
      "epoch: 0, batch: 1500, loss: 5.2256879806518555\n",
      "epoch: 0, batch: 1600, loss: 5.148536205291748\n",
      "epoch: 0, batch: 1700, loss: 5.162995338439941\n",
      "epoch: 0, batch: 1800, loss: 4.954986095428467\n",
      "epoch: 0, batch: 1900, loss: 5.076114654541016\n",
      "epoch: 0, batch: 2000, loss: 5.080732822418213\n",
      "epoch: 0, average loss: 5.502329933877085\n",
      "epoch: 1, batch: 0, loss: 4.980199813842773\n",
      "epoch: 1, batch: 100, loss: 4.963876247406006\n",
      "epoch: 1, batch: 200, loss: 4.879894733428955\n",
      "epoch: 1, batch: 300, loss: 4.8613600730896\n",
      "epoch: 1, batch: 400, loss: 4.948028087615967\n",
      "epoch: 1, batch: 500, loss: 4.982515811920166\n",
      "epoch: 1, batch: 600, loss: 4.958498477935791\n",
      "epoch: 1, batch: 700, loss: 4.906360149383545\n",
      "epoch: 1, batch: 800, loss: 4.898869037628174\n",
      "epoch: 1, batch: 900, loss: 4.845953464508057\n",
      "epoch: 1, batch: 1000, loss: 4.647634029388428\n",
      "epoch: 1, batch: 1100, loss: 4.900487422943115\n",
      "epoch: 1, batch: 1200, loss: 4.906728267669678\n",
      "epoch: 1, batch: 1300, loss: 4.802213191986084\n",
      "epoch: 1, batch: 1400, loss: 4.886674880981445\n",
      "epoch: 1, batch: 1500, loss: 4.753904819488525\n",
      "epoch: 1, batch: 1600, loss: 4.829312801361084\n",
      "epoch: 1, batch: 1700, loss: 4.750105857849121\n",
      "epoch: 1, batch: 1800, loss: 4.825634956359863\n",
      "epoch: 1, batch: 1900, loss: 4.5840935707092285\n",
      "epoch: 1, batch: 2000, loss: 4.670050144195557\n",
      "epoch: 1, average loss: 4.847425479281182\n",
      "epoch: 2, batch: 0, loss: 4.577929496765137\n",
      "epoch: 2, batch: 100, loss: 4.4095964431762695\n",
      "epoch: 2, batch: 200, loss: 4.617525100708008\n",
      "epoch: 2, batch: 300, loss: 4.613575458526611\n",
      "epoch: 2, batch: 400, loss: 4.631083965301514\n",
      "epoch: 2, batch: 500, loss: 4.591416358947754\n",
      "epoch: 2, batch: 600, loss: 4.54740571975708\n",
      "epoch: 2, batch: 700, loss: 4.525359153747559\n",
      "epoch: 2, batch: 800, loss: 4.662348747253418\n",
      "epoch: 2, batch: 900, loss: 4.632879734039307\n",
      "epoch: 2, batch: 1000, loss: 4.5393385887146\n",
      "epoch: 2, batch: 1100, loss: 4.574108123779297\n",
      "epoch: 2, batch: 1200, loss: 4.561397552490234\n",
      "epoch: 2, batch: 1300, loss: 4.476668834686279\n",
      "epoch: 2, batch: 1400, loss: 4.4463419914245605\n",
      "epoch: 2, batch: 1500, loss: 4.597353935241699\n",
      "epoch: 2, batch: 1600, loss: 4.6267900466918945\n",
      "epoch: 2, batch: 1700, loss: 4.597312927246094\n",
      "epoch: 2, batch: 1800, loss: 4.438889980316162\n",
      "epoch: 2, batch: 1900, loss: 4.369425296783447\n",
      "epoch: 2, batch: 2000, loss: 4.460250377655029\n",
      "epoch: 2, average loss: 4.547225336233775\n",
      "epoch: 3, batch: 0, loss: 4.427483081817627\n",
      "epoch: 3, batch: 100, loss: 4.244985103607178\n",
      "epoch: 3, batch: 200, loss: 4.23057222366333\n",
      "epoch: 3, batch: 300, loss: 4.31239652633667\n",
      "epoch: 3, batch: 400, loss: 4.410421848297119\n",
      "epoch: 3, batch: 500, loss: 4.359272480010986\n",
      "epoch: 3, batch: 600, loss: 4.313766002655029\n",
      "epoch: 3, batch: 700, loss: 4.439087390899658\n",
      "epoch: 3, batch: 800, loss: 4.312075138092041\n",
      "epoch: 3, batch: 900, loss: 4.417294979095459\n",
      "epoch: 3, batch: 1000, loss: 4.283586025238037\n",
      "epoch: 3, batch: 1100, loss: 4.32513952255249\n",
      "epoch: 3, batch: 1200, loss: 4.204032897949219\n",
      "epoch: 3, batch: 1300, loss: 4.250788688659668\n",
      "epoch: 3, batch: 1400, loss: 4.2725114822387695\n",
      "epoch: 3, batch: 1500, loss: 4.308574199676514\n",
      "epoch: 3, batch: 1600, loss: 4.29722785949707\n",
      "epoch: 3, batch: 1700, loss: 4.40214729309082\n",
      "epoch: 3, batch: 1800, loss: 4.2265849113464355\n",
      "epoch: 3, batch: 1900, loss: 4.483566761016846\n",
      "epoch: 3, batch: 2000, loss: 4.398441791534424\n",
      "epoch: 3, average loss: 4.297688438144385\n",
      "epoch: 4, batch: 0, loss: 3.9999914169311523\n",
      "epoch: 4, batch: 100, loss: 3.853445291519165\n",
      "epoch: 4, batch: 200, loss: 4.081751346588135\n",
      "epoch: 4, batch: 300, loss: 3.9528706073760986\n",
      "epoch: 4, batch: 400, loss: 4.0425333976745605\n",
      "epoch: 4, batch: 500, loss: 3.9826881885528564\n",
      "epoch: 4, batch: 600, loss: 4.116758346557617\n",
      "epoch: 4, batch: 700, loss: 3.8137848377227783\n",
      "epoch: 4, batch: 800, loss: 4.0363287925720215\n",
      "epoch: 4, batch: 900, loss: 4.05723237991333\n",
      "epoch: 4, batch: 1000, loss: 4.108165264129639\n",
      "epoch: 4, batch: 1100, loss: 4.137690544128418\n",
      "epoch: 4, batch: 1200, loss: 3.94391131401062\n",
      "epoch: 4, batch: 1300, loss: 4.021859169006348\n",
      "epoch: 4, batch: 1400, loss: 4.019942283630371\n",
      "epoch: 4, batch: 1500, loss: 4.191104888916016\n",
      "epoch: 4, batch: 1600, loss: 4.1450724601745605\n",
      "epoch: 4, batch: 1700, loss: 4.062855243682861\n",
      "epoch: 4, batch: 1800, loss: 4.0699005126953125\n",
      "epoch: 4, batch: 1900, loss: 4.0519843101501465\n",
      "epoch: 4, batch: 2000, loss: 4.2686614990234375\n",
      "epoch: 4, average loss: 4.038600779631558\n",
      "epoch: 5, batch: 0, loss: 3.6291885375976562\n",
      "epoch: 5, batch: 100, loss: 3.695546865463257\n",
      "epoch: 5, batch: 200, loss: 3.483083724975586\n",
      "epoch: 5, batch: 300, loss: 3.7096993923187256\n",
      "epoch: 5, batch: 400, loss: 3.6700408458709717\n",
      "epoch: 5, batch: 500, loss: 3.6028213500976562\n",
      "epoch: 5, batch: 600, loss: 3.7967522144317627\n",
      "epoch: 5, batch: 700, loss: 3.7400786876678467\n",
      "epoch: 5, batch: 800, loss: 3.7720372676849365\n",
      "epoch: 5, batch: 900, loss: 3.7390356063842773\n",
      "epoch: 5, batch: 1000, loss: 3.936438798904419\n",
      "epoch: 5, batch: 1100, loss: 3.783325433731079\n",
      "epoch: 5, batch: 1200, loss: 3.8657407760620117\n",
      "epoch: 5, batch: 1300, loss: 3.8422625064849854\n",
      "epoch: 5, batch: 1400, loss: 3.9210851192474365\n",
      "epoch: 5, batch: 1500, loss: 4.034366607666016\n",
      "epoch: 5, batch: 1600, loss: 3.7973527908325195\n",
      "epoch: 5, batch: 1700, loss: 3.81964111328125\n",
      "epoch: 5, batch: 1800, loss: 3.7946290969848633\n",
      "epoch: 5, batch: 1900, loss: 3.8543241024017334\n",
      "epoch: 5, batch: 2000, loss: 3.72082781791687\n",
      "epoch: 5, average loss: 3.7690728234309776\n",
      "epoch: 6, batch: 0, loss: 3.261402130126953\n",
      "epoch: 6, batch: 100, loss: 3.233506202697754\n",
      "epoch: 6, batch: 200, loss: 3.4530298709869385\n",
      "epoch: 6, batch: 300, loss: 3.324873685836792\n",
      "epoch: 6, batch: 400, loss: 3.4147942066192627\n",
      "epoch: 6, batch: 500, loss: 3.530341863632202\n",
      "epoch: 6, batch: 600, loss: 3.666433572769165\n",
      "epoch: 6, batch: 700, loss: 3.4243087768554688\n",
      "epoch: 6, batch: 800, loss: 3.579012870788574\n",
      "epoch: 6, batch: 900, loss: 3.421671152114868\n",
      "epoch: 6, batch: 1000, loss: 3.4072840213775635\n",
      "epoch: 6, batch: 1100, loss: 3.726511001586914\n",
      "epoch: 6, batch: 1200, loss: 3.458033561706543\n",
      "epoch: 6, batch: 1300, loss: 3.3453786373138428\n",
      "epoch: 6, batch: 1400, loss: 3.583700180053711\n",
      "epoch: 6, batch: 1500, loss: 3.4365074634552\n",
      "epoch: 6, batch: 1600, loss: 3.354842185974121\n",
      "epoch: 6, batch: 1700, loss: 3.591125726699829\n",
      "epoch: 6, batch: 1800, loss: 3.6281368732452393\n",
      "epoch: 6, batch: 1900, loss: 3.344123601913452\n",
      "epoch: 6, batch: 2000, loss: 3.636117696762085\n",
      "epoch: 6, average loss: 3.509044549044441\n",
      "epoch: 7, batch: 0, loss: 3.29082989692688\n",
      "epoch: 7, batch: 100, loss: 3.051234483718872\n",
      "epoch: 7, batch: 200, loss: 3.0976336002349854\n",
      "epoch: 7, batch: 300, loss: 3.2538254261016846\n",
      "epoch: 7, batch: 400, loss: 3.1295480728149414\n",
      "epoch: 7, batch: 500, loss: 3.2834088802337646\n",
      "epoch: 7, batch: 600, loss: 3.2798919677734375\n",
      "epoch: 7, batch: 700, loss: 3.2524116039276123\n",
      "epoch: 7, batch: 800, loss: 3.3254013061523438\n",
      "epoch: 7, batch: 900, loss: 3.2820661067962646\n",
      "epoch: 7, batch: 1000, loss: 3.34071946144104\n",
      "epoch: 7, batch: 1100, loss: 3.1742050647735596\n",
      "epoch: 7, batch: 1200, loss: 3.4409186840057373\n",
      "epoch: 7, batch: 1300, loss: 3.193532943725586\n",
      "epoch: 7, batch: 1400, loss: 3.369807004928589\n",
      "epoch: 7, batch: 1500, loss: 3.3314123153686523\n",
      "epoch: 7, batch: 1600, loss: 3.2328579425811768\n",
      "epoch: 7, batch: 1700, loss: 3.3379695415496826\n",
      "epoch: 7, batch: 1800, loss: 3.303278923034668\n",
      "epoch: 7, batch: 1900, loss: 3.495326042175293\n",
      "epoch: 7, batch: 2000, loss: 3.374279737472534\n",
      "epoch: 7, average loss: 3.2717827100379795\n",
      "epoch: 8, batch: 0, loss: 2.7876036167144775\n",
      "epoch: 8, batch: 100, loss: 2.809528112411499\n",
      "epoch: 8, batch: 200, loss: 2.7927768230438232\n",
      "epoch: 8, batch: 300, loss: 3.061175584793091\n",
      "epoch: 8, batch: 400, loss: 2.9007623195648193\n",
      "epoch: 8, batch: 500, loss: 2.9044291973114014\n",
      "epoch: 8, batch: 600, loss: 3.0994863510131836\n",
      "epoch: 8, batch: 700, loss: 3.0976693630218506\n",
      "epoch: 8, batch: 800, loss: 3.067035675048828\n",
      "epoch: 8, batch: 900, loss: 3.092496633529663\n",
      "epoch: 8, batch: 1000, loss: 3.1547000408172607\n",
      "epoch: 8, batch: 1100, loss: 3.033114433288574\n",
      "epoch: 8, batch: 1200, loss: 3.245635747909546\n",
      "epoch: 8, batch: 1300, loss: 3.2565858364105225\n",
      "epoch: 8, batch: 1400, loss: 3.303684949874878\n",
      "epoch: 8, batch: 1500, loss: 3.2775392532348633\n",
      "epoch: 8, batch: 1600, loss: 3.1727075576782227\n",
      "epoch: 8, batch: 1700, loss: 3.2139453887939453\n",
      "epoch: 8, batch: 1800, loss: 3.246565580368042\n",
      "epoch: 8, batch: 1900, loss: 3.090669870376587\n",
      "epoch: 8, batch: 2000, loss: 3.123213768005371\n",
      "epoch: 8, average loss: 3.063389235384324\n",
      "epoch: 9, batch: 0, loss: 2.705449104309082\n",
      "epoch: 9, batch: 100, loss: 2.6850998401641846\n",
      "epoch: 9, batch: 200, loss: 2.518432855606079\n",
      "epoch: 9, batch: 300, loss: 2.6443161964416504\n",
      "epoch: 9, batch: 400, loss: 2.769038438796997\n",
      "epoch: 9, batch: 500, loss: 2.733370065689087\n",
      "epoch: 9, batch: 600, loss: 2.6979570388793945\n",
      "epoch: 9, batch: 700, loss: 2.832918882369995\n",
      "epoch: 9, batch: 800, loss: 2.7013776302337646\n",
      "epoch: 9, batch: 900, loss: 3.032461166381836\n",
      "epoch: 9, batch: 1000, loss: 2.8157403469085693\n",
      "epoch: 9, batch: 1100, loss: 2.8086156845092773\n",
      "epoch: 9, batch: 1200, loss: 2.9660627841949463\n",
      "epoch: 9, batch: 1300, loss: 3.098146677017212\n",
      "epoch: 9, batch: 1400, loss: 2.998638153076172\n",
      "epoch: 9, batch: 1500, loss: 2.987088203430176\n",
      "epoch: 9, batch: 1600, loss: 3.1166632175445557\n",
      "epoch: 9, batch: 1700, loss: 2.724588632583618\n",
      "epoch: 9, batch: 1800, loss: 3.0754337310791016\n",
      "epoch: 9, batch: 1900, loss: 2.989384889602661\n",
      "epoch: 9, batch: 2000, loss: 2.966811180114746\n",
      "epoch: 9, average loss: 2.879871318620794\n",
      "epoch: 10, batch: 0, loss: 2.5512826442718506\n",
      "epoch: 10, batch: 100, loss: 2.3471715450286865\n",
      "epoch: 10, batch: 200, loss: 2.333479881286621\n",
      "epoch: 10, batch: 300, loss: 2.3140408992767334\n",
      "epoch: 10, batch: 400, loss: 2.145270347595215\n",
      "epoch: 10, batch: 500, loss: 2.114380121231079\n",
      "epoch: 10, batch: 600, loss: 2.2405331134796143\n",
      "epoch: 10, batch: 700, loss: 2.1846699714660645\n",
      "epoch: 10, batch: 800, loss: 2.2779970169067383\n",
      "epoch: 10, batch: 900, loss: 2.1951730251312256\n",
      "epoch: 10, batch: 1000, loss: 2.371199369430542\n",
      "epoch: 10, batch: 1100, loss: 2.255408525466919\n",
      "epoch: 10, batch: 1200, loss: 2.413701295852661\n",
      "epoch: 10, batch: 1300, loss: 2.2176644802093506\n",
      "epoch: 10, batch: 1400, loss: 2.255368947982788\n",
      "epoch: 10, batch: 1500, loss: 2.2195966243743896\n",
      "epoch: 10, batch: 1600, loss: 2.228980779647827\n",
      "epoch: 10, batch: 1700, loss: 2.129411458969116\n",
      "epoch: 10, batch: 1800, loss: 2.0718069076538086\n",
      "epoch: 10, batch: 1900, loss: 2.1785738468170166\n",
      "epoch: 10, batch: 2000, loss: 2.193964719772339\n",
      "epoch: 10, average loss: 2.2259773179596545\n",
      "epoch: 11, batch: 0, loss: 1.975072979927063\n",
      "epoch: 11, batch: 100, loss: 2.007988214492798\n",
      "epoch: 11, batch: 200, loss: 1.9749006032943726\n",
      "epoch: 11, batch: 300, loss: 1.9707175493240356\n",
      "epoch: 11, batch: 400, loss: 1.954595923423767\n",
      "epoch: 11, batch: 500, loss: 1.962583065032959\n",
      "epoch: 11, batch: 600, loss: 1.9559383392333984\n",
      "epoch: 11, batch: 700, loss: 1.678200364112854\n",
      "epoch: 11, batch: 800, loss: 2.1424922943115234\n",
      "epoch: 11, batch: 900, loss: 2.1119534969329834\n",
      "epoch: 11, batch: 1000, loss: 1.998121738433838\n",
      "epoch: 11, batch: 1100, loss: 1.9541311264038086\n",
      "epoch: 11, batch: 1200, loss: 2.082278251647949\n",
      "epoch: 11, batch: 1300, loss: 1.8473707437515259\n",
      "epoch: 11, batch: 1400, loss: 2.08963680267334\n",
      "epoch: 11, batch: 1500, loss: 1.9366636276245117\n",
      "epoch: 11, batch: 1600, loss: 2.0131797790527344\n",
      "epoch: 11, batch: 1700, loss: 1.9046636819839478\n",
      "epoch: 11, batch: 1800, loss: 1.7696536779403687\n",
      "epoch: 11, batch: 1900, loss: 1.9859485626220703\n",
      "epoch: 11, batch: 2000, loss: 1.9285491704940796\n",
      "epoch: 11, average loss: 1.9731999626931023\n",
      "epoch: 12, batch: 0, loss: 1.8959087133407593\n",
      "epoch: 12, batch: 100, loss: 1.7972663640975952\n",
      "epoch: 12, batch: 200, loss: 1.9190531969070435\n",
      "epoch: 12, batch: 300, loss: 1.8349031209945679\n",
      "epoch: 12, batch: 400, loss: 1.7613487243652344\n",
      "epoch: 12, batch: 500, loss: 1.7317177057266235\n",
      "epoch: 12, batch: 600, loss: 1.766846776008606\n",
      "epoch: 12, batch: 700, loss: 1.8324966430664062\n",
      "epoch: 12, batch: 800, loss: 1.8306989669799805\n",
      "epoch: 12, batch: 900, loss: 1.8997522592544556\n",
      "epoch: 12, batch: 1000, loss: 1.9163898229599\n",
      "epoch: 12, batch: 1100, loss: 1.8750032186508179\n",
      "epoch: 12, batch: 1200, loss: 1.8292427062988281\n",
      "epoch: 12, batch: 1300, loss: 1.8040393590927124\n",
      "epoch: 12, batch: 1400, loss: 1.6295744180679321\n",
      "epoch: 12, batch: 1500, loss: 1.9246314764022827\n",
      "epoch: 12, batch: 1600, loss: 1.909089207649231\n",
      "epoch: 12, batch: 1700, loss: 1.7216061353683472\n",
      "epoch: 12, batch: 1800, loss: 1.8258479833602905\n",
      "epoch: 12, batch: 1900, loss: 1.8954488039016724\n",
      "epoch: 12, batch: 2000, loss: 1.8284248113632202\n",
      "epoch: 12, average loss: 1.8199129321995904\n",
      "epoch: 13, batch: 0, loss: 1.6989659070968628\n",
      "epoch: 13, batch: 100, loss: 1.7046908140182495\n",
      "epoch: 13, batch: 200, loss: 1.79964017868042\n",
      "epoch: 13, batch: 300, loss: 1.7366329431533813\n",
      "epoch: 13, batch: 400, loss: 1.6072463989257812\n",
      "epoch: 13, batch: 500, loss: 1.6403611898422241\n",
      "epoch: 13, batch: 600, loss: 1.6372922658920288\n",
      "epoch: 13, batch: 700, loss: 1.6041556596755981\n",
      "epoch: 13, batch: 800, loss: 1.78671395778656\n",
      "epoch: 13, batch: 900, loss: 1.5800647735595703\n",
      "epoch: 13, batch: 1000, loss: 1.7199158668518066\n",
      "epoch: 13, batch: 1100, loss: 1.5986310243606567\n",
      "epoch: 13, batch: 1200, loss: 1.688501238822937\n",
      "epoch: 13, batch: 1300, loss: 1.611173152923584\n",
      "epoch: 13, batch: 1400, loss: 1.8268723487854004\n",
      "epoch: 13, batch: 1500, loss: 1.6312541961669922\n",
      "epoch: 13, batch: 1600, loss: 1.7597590684890747\n",
      "epoch: 13, batch: 1700, loss: 1.7732443809509277\n",
      "epoch: 13, batch: 1800, loss: 1.800310730934143\n",
      "epoch: 13, batch: 1900, loss: 1.7749629020690918\n",
      "epoch: 13, batch: 2000, loss: 1.6436198949813843\n",
      "epoch: 13, average loss: 1.690321049211072\n",
      "epoch: 14, batch: 0, loss: 1.3918365240097046\n",
      "epoch: 14, batch: 100, loss: 1.4827817678451538\n",
      "epoch: 14, batch: 200, loss: 1.4785760641098022\n",
      "epoch: 14, batch: 300, loss: 1.4959899187088013\n",
      "epoch: 14, batch: 400, loss: 1.4953187704086304\n",
      "epoch: 14, batch: 500, loss: 1.5689729452133179\n",
      "epoch: 14, batch: 600, loss: 1.57856285572052\n",
      "epoch: 14, batch: 700, loss: 1.571731686592102\n",
      "epoch: 14, batch: 800, loss: 1.5028377771377563\n",
      "epoch: 14, batch: 900, loss: 1.5357465744018555\n",
      "epoch: 14, batch: 1000, loss: 1.800195574760437\n",
      "epoch: 14, batch: 1100, loss: 1.6490864753723145\n",
      "epoch: 14, batch: 1200, loss: 1.6890672445297241\n",
      "epoch: 14, batch: 1300, loss: 1.54015052318573\n",
      "epoch: 14, batch: 1400, loss: 1.5283523797988892\n",
      "epoch: 14, batch: 1500, loss: 1.5294679403305054\n",
      "epoch: 14, batch: 1600, loss: 1.5739868879318237\n",
      "epoch: 14, batch: 1700, loss: 1.4689431190490723\n",
      "epoch: 14, batch: 1800, loss: 1.768011212348938\n",
      "epoch: 14, batch: 1900, loss: 1.7477421760559082\n",
      "epoch: 14, batch: 2000, loss: 1.4943456649780273\n",
      "epoch: 14, average loss: 1.5731367990082386\n",
      "epoch: 15, batch: 0, loss: 1.312059760093689\n",
      "epoch: 15, batch: 100, loss: 1.3744688034057617\n",
      "epoch: 15, batch: 200, loss: 1.3898223638534546\n",
      "epoch: 15, batch: 300, loss: 1.3114813566207886\n",
      "epoch: 15, batch: 400, loss: 1.5193217992782593\n",
      "epoch: 15, batch: 500, loss: 1.3406037092208862\n",
      "epoch: 15, batch: 600, loss: 1.513076663017273\n",
      "epoch: 15, batch: 700, loss: 1.398120403289795\n",
      "epoch: 15, batch: 800, loss: 1.537917137145996\n",
      "epoch: 15, batch: 900, loss: 1.5557031631469727\n",
      "epoch: 15, batch: 1000, loss: 1.3961620330810547\n",
      "epoch: 15, batch: 1100, loss: 1.4754977226257324\n",
      "epoch: 15, batch: 1200, loss: 1.485674262046814\n",
      "epoch: 15, batch: 1300, loss: 1.545076847076416\n",
      "epoch: 15, batch: 1400, loss: 1.458748698234558\n",
      "epoch: 15, batch: 1500, loss: 1.4698435068130493\n",
      "epoch: 15, batch: 1600, loss: 1.4366649389266968\n",
      "epoch: 15, batch: 1700, loss: 1.475779414176941\n",
      "epoch: 15, batch: 1800, loss: 1.6206001043319702\n",
      "epoch: 15, batch: 1900, loss: 1.481610894203186\n",
      "epoch: 15, batch: 2000, loss: 1.4826955795288086\n",
      "epoch: 15, average loss: 1.465009754896164\n",
      "epoch: 16, batch: 0, loss: 1.351148009300232\n",
      "epoch: 16, batch: 100, loss: 1.3742871284484863\n",
      "epoch: 16, batch: 200, loss: 1.3294516801834106\n",
      "epoch: 16, batch: 300, loss: 1.2107789516448975\n",
      "epoch: 16, batch: 400, loss: 1.4161667823791504\n",
      "epoch: 16, batch: 500, loss: 1.3614076375961304\n",
      "epoch: 16, batch: 600, loss: 1.3904505968093872\n",
      "epoch: 16, batch: 700, loss: 1.3452256917953491\n",
      "epoch: 16, batch: 800, loss: 1.2864325046539307\n",
      "epoch: 16, batch: 900, loss: 1.306462049484253\n",
      "epoch: 16, batch: 1000, loss: 1.306331753730774\n",
      "epoch: 16, batch: 1100, loss: 1.381593108177185\n",
      "epoch: 16, batch: 1200, loss: 1.2555257081985474\n",
      "epoch: 16, batch: 1300, loss: 1.324447512626648\n",
      "epoch: 16, batch: 1400, loss: 1.3793812990188599\n",
      "epoch: 16, batch: 1500, loss: 1.3905144929885864\n",
      "epoch: 16, batch: 1600, loss: 1.4142028093338013\n",
      "epoch: 16, batch: 1700, loss: 1.3551274538040161\n",
      "epoch: 16, batch: 1800, loss: 1.3986207246780396\n",
      "epoch: 16, batch: 1900, loss: 1.3158279657363892\n",
      "epoch: 16, batch: 2000, loss: 1.360214114189148\n",
      "epoch: 16, average loss: 1.3645521672332988\n",
      "epoch: 17, batch: 0, loss: 1.1645022630691528\n",
      "epoch: 17, batch: 100, loss: 1.2766395807266235\n",
      "epoch: 17, batch: 200, loss: 1.2055357694625854\n",
      "epoch: 17, batch: 300, loss: 1.2323744297027588\n",
      "epoch: 17, batch: 400, loss: 1.1735397577285767\n",
      "epoch: 17, batch: 500, loss: 1.1956065893173218\n",
      "epoch: 17, batch: 600, loss: 1.1775856018066406\n",
      "epoch: 17, batch: 700, loss: 1.2709722518920898\n",
      "epoch: 17, batch: 800, loss: 1.2549176216125488\n",
      "epoch: 17, batch: 900, loss: 1.2543026208877563\n",
      "epoch: 17, batch: 1000, loss: 1.2712106704711914\n",
      "epoch: 17, batch: 1100, loss: 1.1612529754638672\n",
      "epoch: 17, batch: 1200, loss: 1.3296518325805664\n",
      "epoch: 17, batch: 1300, loss: 1.2555660009384155\n",
      "epoch: 17, batch: 1400, loss: 1.2441192865371704\n",
      "epoch: 17, batch: 1500, loss: 1.4354435205459595\n",
      "epoch: 17, batch: 1600, loss: 1.370223045349121\n",
      "epoch: 17, batch: 1700, loss: 1.315006971359253\n",
      "epoch: 17, batch: 1800, loss: 1.3539915084838867\n",
      "epoch: 17, batch: 1900, loss: 1.3153562545776367\n",
      "epoch: 17, batch: 2000, loss: 1.3216662406921387\n",
      "epoch: 17, average loss: 1.2713975205140955\n",
      "epoch: 18, batch: 0, loss: 1.0642443895339966\n",
      "epoch: 18, batch: 100, loss: 1.2367626428604126\n",
      "epoch: 18, batch: 200, loss: 1.1564695835113525\n",
      "epoch: 18, batch: 300, loss: 1.119296669960022\n",
      "epoch: 18, batch: 400, loss: 1.1214003562927246\n",
      "epoch: 18, batch: 500, loss: 1.2314319610595703\n",
      "epoch: 18, batch: 600, loss: 1.163106918334961\n",
      "epoch: 18, batch: 700, loss: 1.3125280141830444\n",
      "epoch: 18, batch: 800, loss: 1.303916573524475\n",
      "epoch: 18, batch: 900, loss: 1.1213582754135132\n",
      "epoch: 18, batch: 1000, loss: 1.1728774309158325\n",
      "epoch: 18, batch: 1100, loss: 1.1907240152359009\n",
      "epoch: 18, batch: 1200, loss: 1.2126588821411133\n",
      "epoch: 18, batch: 1300, loss: 1.2086740732192993\n",
      "epoch: 18, batch: 1400, loss: 1.1458494663238525\n",
      "epoch: 18, batch: 1500, loss: 1.1764662265777588\n",
      "epoch: 18, batch: 1600, loss: 1.2173465490341187\n",
      "epoch: 18, batch: 1700, loss: 1.3622781038284302\n",
      "epoch: 18, batch: 1800, loss: 1.1670335531234741\n",
      "epoch: 18, batch: 1900, loss: 1.1139622926712036\n",
      "epoch: 18, batch: 2000, loss: 1.2587825059890747\n",
      "epoch: 18, average loss: 1.1848760732540897\n",
      "epoch: 19, batch: 0, loss: 1.059449315071106\n",
      "epoch: 19, batch: 100, loss: 1.15172278881073\n",
      "epoch: 19, batch: 200, loss: 1.0318752527236938\n",
      "epoch: 19, batch: 300, loss: 1.0731240510940552\n",
      "epoch: 19, batch: 400, loss: 1.124658465385437\n",
      "epoch: 19, batch: 500, loss: 1.1064120531082153\n",
      "epoch: 19, batch: 600, loss: 1.1218348741531372\n",
      "epoch: 19, batch: 700, loss: 1.1074713468551636\n",
      "epoch: 19, batch: 800, loss: 1.0646889209747314\n",
      "epoch: 19, batch: 900, loss: 1.1280683279037476\n",
      "epoch: 19, batch: 1000, loss: 0.9969727396965027\n",
      "epoch: 19, batch: 1100, loss: 1.1816195249557495\n",
      "epoch: 19, batch: 1200, loss: 1.1112900972366333\n",
      "epoch: 19, batch: 1300, loss: 1.2471778392791748\n",
      "epoch: 19, batch: 1400, loss: 1.2021335363388062\n",
      "epoch: 19, batch: 1500, loss: 1.0907098054885864\n",
      "epoch: 19, batch: 1600, loss: 1.0323810577392578\n",
      "epoch: 19, batch: 1700, loss: 1.111425757408142\n",
      "epoch: 19, batch: 1800, loss: 1.1632264852523804\n",
      "epoch: 19, batch: 1900, loss: 1.0808771848678589\n",
      "epoch: 19, batch: 2000, loss: 1.208918809890747\n",
      "epoch: 19, average loss: 1.1055957277323685\n",
      "epoch: 20, batch: 0, loss: 1.033615231513977\n",
      "epoch: 20, batch: 100, loss: 1.0371061563491821\n",
      "epoch: 20, batch: 200, loss: 0.9287502765655518\n",
      "epoch: 20, batch: 300, loss: 0.9407487511634827\n",
      "epoch: 20, batch: 400, loss: 0.9517615437507629\n",
      "epoch: 20, batch: 500, loss: 0.9257559180259705\n",
      "epoch: 20, batch: 600, loss: 0.9206954836845398\n",
      "epoch: 20, batch: 700, loss: 0.9753292202949524\n",
      "epoch: 20, batch: 800, loss: 1.036160945892334\n",
      "epoch: 20, batch: 900, loss: 0.9087308049201965\n",
      "epoch: 20, batch: 1000, loss: 1.0434596538543701\n",
      "epoch: 20, batch: 1100, loss: 0.9897467494010925\n",
      "epoch: 20, batch: 1200, loss: 0.8898323178291321\n",
      "epoch: 20, batch: 1300, loss: 0.9809690117835999\n",
      "epoch: 20, batch: 1400, loss: 1.061566948890686\n",
      "epoch: 20, batch: 1500, loss: 0.8277029395103455\n",
      "epoch: 20, batch: 1600, loss: 0.9436870217323303\n",
      "epoch: 20, batch: 1700, loss: 1.0717949867248535\n",
      "epoch: 20, batch: 1800, loss: 0.9188917279243469\n",
      "epoch: 20, batch: 1900, loss: 0.9113407135009766\n",
      "epoch: 20, batch: 2000, loss: 1.0074931383132935\n",
      "epoch: 20, average loss: 0.9740043079444006\n",
      "epoch: 21, batch: 0, loss: 0.9648162722587585\n",
      "epoch: 21, batch: 100, loss: 0.9691291451454163\n",
      "epoch: 21, batch: 200, loss: 0.9468955993652344\n",
      "epoch: 21, batch: 300, loss: 0.9123809337615967\n",
      "epoch: 21, batch: 400, loss: 0.9599694609642029\n",
      "epoch: 21, batch: 500, loss: 0.9397043585777283\n",
      "epoch: 21, batch: 600, loss: 0.9877681732177734\n",
      "epoch: 21, batch: 700, loss: 0.934377133846283\n",
      "epoch: 21, batch: 800, loss: 0.814337968826294\n",
      "epoch: 21, batch: 900, loss: 0.9510756134986877\n",
      "epoch: 21, batch: 1000, loss: 0.9958134293556213\n",
      "epoch: 21, batch: 1100, loss: 0.9038507342338562\n",
      "epoch: 21, batch: 1200, loss: 0.942897379398346\n",
      "epoch: 21, batch: 1300, loss: 0.9058935642242432\n",
      "epoch: 21, batch: 1400, loss: 0.9226743578910828\n",
      "epoch: 21, batch: 1500, loss: 0.9972884654998779\n",
      "epoch: 21, batch: 1600, loss: 0.8955802321434021\n",
      "epoch: 21, batch: 1700, loss: 0.9823503494262695\n",
      "epoch: 21, batch: 1800, loss: 0.9559710621833801\n",
      "epoch: 21, batch: 1900, loss: 0.9174999594688416\n",
      "epoch: 21, batch: 2000, loss: 0.9885962605476379\n",
      "epoch: 21, average loss: 0.9524914479138804\n",
      "epoch: 22, batch: 0, loss: 0.8946033120155334\n",
      "epoch: 22, batch: 100, loss: 0.9534707069396973\n",
      "epoch: 22, batch: 200, loss: 1.0094150304794312\n",
      "epoch: 22, batch: 300, loss: 0.8532552719116211\n",
      "epoch: 22, batch: 400, loss: 0.9796535968780518\n",
      "epoch: 22, batch: 500, loss: 0.9497062563896179\n",
      "epoch: 22, batch: 600, loss: 1.0644938945770264\n",
      "epoch: 22, batch: 700, loss: 0.9819996356964111\n",
      "epoch: 22, batch: 800, loss: 0.984184980392456\n",
      "epoch: 22, batch: 900, loss: 0.9255347847938538\n",
      "epoch: 22, batch: 1000, loss: 0.927156925201416\n",
      "epoch: 22, batch: 1100, loss: 0.8845975995063782\n",
      "epoch: 22, batch: 1200, loss: 0.8682599067687988\n",
      "epoch: 22, batch: 1300, loss: 0.934077799320221\n",
      "epoch: 22, batch: 1400, loss: 0.9158836007118225\n",
      "epoch: 22, batch: 1500, loss: 0.9030790328979492\n",
      "epoch: 22, batch: 1600, loss: 1.0173135995864868\n",
      "epoch: 22, batch: 1700, loss: 0.9969729781150818\n",
      "epoch: 22, batch: 1800, loss: 0.898087203502655\n",
      "epoch: 22, batch: 1900, loss: 0.8997375965118408\n",
      "epoch: 22, batch: 2000, loss: 0.9003730416297913\n",
      "epoch: 22, average loss: 0.9366688386190172\n",
      "epoch: 23, batch: 0, loss: 0.9017045497894287\n",
      "epoch: 23, batch: 100, loss: 0.9015948176383972\n",
      "epoch: 23, batch: 200, loss: 0.8773612976074219\n",
      "epoch: 23, batch: 300, loss: 1.0034948587417603\n",
      "epoch: 23, batch: 400, loss: 0.8662354946136475\n",
      "epoch: 23, batch: 500, loss: 0.9596311450004578\n",
      "epoch: 23, batch: 600, loss: 0.8991979956626892\n",
      "epoch: 23, batch: 700, loss: 0.9188783764839172\n",
      "epoch: 23, batch: 800, loss: 0.8757214546203613\n",
      "epoch: 23, batch: 900, loss: 0.8952919840812683\n",
      "epoch: 23, batch: 1000, loss: 0.9899485111236572\n",
      "epoch: 23, batch: 1100, loss: 0.9390060901641846\n",
      "epoch: 23, batch: 1200, loss: 0.9026195406913757\n",
      "epoch: 23, batch: 1300, loss: 0.9126226305961609\n",
      "epoch: 23, batch: 1400, loss: 0.8843140602111816\n",
      "epoch: 23, batch: 1500, loss: 1.0057672262191772\n",
      "epoch: 23, batch: 1600, loss: 0.8670730590820312\n",
      "epoch: 23, batch: 1700, loss: 0.8384065628051758\n",
      "epoch: 23, batch: 1800, loss: 0.8267750144004822\n",
      "epoch: 23, batch: 1900, loss: 0.9578656554222107\n",
      "epoch: 23, batch: 2000, loss: 0.9699037671089172\n",
      "epoch: 23, average loss: 0.9219439249997046\n",
      "epoch: 24, batch: 0, loss: 0.8974363207817078\n",
      "epoch: 24, batch: 100, loss: 0.9116718769073486\n",
      "epoch: 24, batch: 200, loss: 0.9602180123329163\n",
      "epoch: 24, batch: 300, loss: 0.9434255957603455\n",
      "epoch: 24, batch: 400, loss: 0.9160812497138977\n",
      "epoch: 24, batch: 500, loss: 0.9087786078453064\n",
      "epoch: 24, batch: 600, loss: 0.8372377753257751\n",
      "epoch: 24, batch: 700, loss: 0.8891599774360657\n",
      "epoch: 24, batch: 800, loss: 0.8583158850669861\n",
      "epoch: 24, batch: 900, loss: 0.8371140956878662\n",
      "epoch: 24, batch: 1000, loss: 0.8879778981208801\n",
      "epoch: 24, batch: 1100, loss: 1.0167683362960815\n",
      "epoch: 24, batch: 1200, loss: 0.8340774178504944\n",
      "epoch: 24, batch: 1300, loss: 0.8825708031654358\n",
      "epoch: 24, batch: 1400, loss: 0.8458343148231506\n",
      "epoch: 24, batch: 1500, loss: 0.9458470344543457\n",
      "epoch: 24, batch: 1600, loss: 0.9728972911834717\n",
      "epoch: 24, batch: 1700, loss: 0.874519407749176\n",
      "epoch: 24, batch: 1800, loss: 0.9115021228790283\n",
      "epoch: 24, batch: 1900, loss: 0.9547763466835022\n",
      "epoch: 24, batch: 2000, loss: 0.9264860153198242\n",
      "epoch: 24, average loss: 0.9080786352064095\n",
      "epoch: 25, batch: 0, loss: 0.9482741355895996\n",
      "epoch: 25, batch: 100, loss: 0.7997643351554871\n",
      "epoch: 25, batch: 200, loss: 0.8984573483467102\n",
      "epoch: 25, batch: 300, loss: 0.8815156817436218\n",
      "epoch: 25, batch: 400, loss: 0.7969958782196045\n",
      "epoch: 25, batch: 500, loss: 0.8624277710914612\n",
      "epoch: 25, batch: 600, loss: 0.7503805160522461\n",
      "epoch: 25, batch: 700, loss: 0.8012656569480896\n",
      "epoch: 25, batch: 800, loss: 0.9964566826820374\n",
      "epoch: 25, batch: 900, loss: 0.9121343493461609\n",
      "epoch: 25, batch: 1000, loss: 1.0236917734146118\n",
      "epoch: 25, batch: 1100, loss: 0.948091983795166\n",
      "epoch: 25, batch: 1200, loss: 0.9277531504631042\n",
      "epoch: 25, batch: 1300, loss: 1.0436357259750366\n",
      "epoch: 25, batch: 1400, loss: 0.8319830894470215\n",
      "epoch: 25, batch: 1500, loss: 0.8098720908164978\n",
      "epoch: 25, batch: 1600, loss: 0.8367509245872498\n",
      "epoch: 25, batch: 1700, loss: 0.9231975078582764\n",
      "epoch: 25, batch: 1800, loss: 0.8877306580543518\n",
      "epoch: 25, batch: 1900, loss: 0.9867760539054871\n",
      "epoch: 25, batch: 2000, loss: 0.955977737903595\n",
      "epoch: 25, average loss: 0.8948087739886023\n",
      "epoch: 26, batch: 0, loss: 0.8072466850280762\n",
      "epoch: 26, batch: 100, loss: 0.868691623210907\n",
      "epoch: 26, batch: 200, loss: 0.8793308734893799\n",
      "epoch: 26, batch: 300, loss: 0.8808699250221252\n",
      "epoch: 26, batch: 400, loss: 0.8598585724830627\n",
      "epoch: 26, batch: 500, loss: 0.9136190414428711\n",
      "epoch: 26, batch: 600, loss: 0.838864803314209\n",
      "epoch: 26, batch: 700, loss: 0.9047936797142029\n",
      "epoch: 26, batch: 800, loss: 0.9415512681007385\n",
      "epoch: 26, batch: 900, loss: 0.8838260769844055\n",
      "epoch: 26, batch: 1000, loss: 0.7809572219848633\n",
      "epoch: 26, batch: 1100, loss: 0.8882343173027039\n",
      "epoch: 26, batch: 1200, loss: 0.906001627445221\n",
      "epoch: 26, batch: 1300, loss: 0.9451835751533508\n",
      "epoch: 26, batch: 1400, loss: 0.9311122894287109\n",
      "epoch: 26, batch: 1500, loss: 0.8198273181915283\n",
      "epoch: 26, batch: 1600, loss: 0.9178809523582458\n",
      "epoch: 26, batch: 1700, loss: 0.8564515709877014\n",
      "epoch: 26, batch: 1800, loss: 0.9340067505836487\n",
      "epoch: 26, batch: 1900, loss: 0.8761886954307556\n",
      "epoch: 26, batch: 2000, loss: 0.8478953242301941\n",
      "epoch: 26, average loss: 0.8820762313171929\n",
      "epoch: 27, batch: 0, loss: 0.8806915879249573\n",
      "epoch: 27, batch: 100, loss: 0.9059901833534241\n",
      "epoch: 27, batch: 200, loss: 0.9303784370422363\n",
      "epoch: 27, batch: 300, loss: 0.8613767027854919\n",
      "epoch: 27, batch: 400, loss: 0.9158008694648743\n",
      "epoch: 27, batch: 500, loss: 0.81083083152771\n",
      "epoch: 27, batch: 600, loss: 0.8091094493865967\n",
      "epoch: 27, batch: 700, loss: 0.8753328919410706\n",
      "epoch: 27, batch: 800, loss: 0.8515364527702332\n",
      "epoch: 27, batch: 900, loss: 0.8815843462944031\n",
      "epoch: 27, batch: 1000, loss: 0.8444981575012207\n",
      "epoch: 27, batch: 1100, loss: 0.7954700589179993\n",
      "epoch: 27, batch: 1200, loss: 1.0003981590270996\n",
      "epoch: 27, batch: 1300, loss: 0.8684451580047607\n",
      "epoch: 27, batch: 1400, loss: 0.7639660239219666\n",
      "epoch: 27, batch: 1500, loss: 0.8985984921455383\n",
      "epoch: 27, batch: 1600, loss: 0.8228804469108582\n",
      "epoch: 27, batch: 1700, loss: 1.0161269903182983\n",
      "epoch: 27, batch: 1800, loss: 0.8046053051948547\n",
      "epoch: 27, batch: 1900, loss: 0.8591781258583069\n",
      "epoch: 27, batch: 2000, loss: 0.836897075176239\n",
      "epoch: 27, average loss: 0.8695794405013907\n",
      "epoch: 28, batch: 0, loss: 0.8684434294700623\n",
      "epoch: 28, batch: 100, loss: 0.9254360198974609\n",
      "epoch: 28, batch: 200, loss: 0.8256397247314453\n",
      "epoch: 28, batch: 300, loss: 0.9010801315307617\n",
      "epoch: 28, batch: 400, loss: 0.8085281848907471\n",
      "epoch: 28, batch: 500, loss: 0.9611249566078186\n",
      "epoch: 28, batch: 600, loss: 0.828222930431366\n",
      "epoch: 28, batch: 700, loss: 0.8301124572753906\n",
      "epoch: 28, batch: 800, loss: 0.9026637077331543\n",
      "epoch: 28, batch: 900, loss: 0.8310359120368958\n",
      "epoch: 28, batch: 1000, loss: 0.8662331104278564\n",
      "epoch: 28, batch: 1100, loss: 0.8126730918884277\n",
      "epoch: 28, batch: 1200, loss: 0.8387910723686218\n",
      "epoch: 28, batch: 1300, loss: 0.8949983716011047\n",
      "epoch: 28, batch: 1400, loss: 0.8420024514198303\n",
      "epoch: 28, batch: 1500, loss: 0.8348243832588196\n",
      "epoch: 28, batch: 1600, loss: 0.8399801850318909\n",
      "epoch: 28, batch: 1700, loss: 0.8027189373970032\n",
      "epoch: 28, batch: 1800, loss: 0.8700580596923828\n",
      "epoch: 28, batch: 1900, loss: 0.818142831325531\n",
      "epoch: 28, batch: 2000, loss: 0.8543358445167542\n",
      "epoch: 28, average loss: 0.857391207709032\n",
      "epoch: 29, batch: 0, loss: 0.9419854283332825\n",
      "epoch: 29, batch: 100, loss: 0.9501096606254578\n",
      "epoch: 29, batch: 200, loss: 0.8389958739280701\n",
      "epoch: 29, batch: 300, loss: 0.8829479813575745\n",
      "epoch: 29, batch: 400, loss: 0.9131212830543518\n",
      "epoch: 29, batch: 500, loss: 0.7719114422798157\n",
      "epoch: 29, batch: 600, loss: 0.8317921757698059\n",
      "epoch: 29, batch: 700, loss: 0.8884238600730896\n",
      "epoch: 29, batch: 800, loss: 0.9315953254699707\n",
      "epoch: 29, batch: 900, loss: 0.806814432144165\n",
      "epoch: 29, batch: 1000, loss: 0.9148752093315125\n",
      "epoch: 29, batch: 1100, loss: 0.8001242280006409\n",
      "epoch: 29, batch: 1200, loss: 0.8536035418510437\n",
      "epoch: 29, batch: 1300, loss: 0.8439292311668396\n",
      "epoch: 29, batch: 1400, loss: 0.8916058540344238\n",
      "epoch: 29, batch: 1500, loss: 0.8817808628082275\n",
      "epoch: 29, batch: 1600, loss: 0.753964364528656\n",
      "epoch: 29, batch: 1700, loss: 0.8192231059074402\n",
      "epoch: 29, batch: 1800, loss: 0.8443982601165771\n",
      "epoch: 29, batch: 1900, loss: 0.7373945713043213\n",
      "epoch: 29, batch: 2000, loss: 0.9157297611236572\n",
      "epoch: 29, average loss: 0.8454698640049673\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = PoetryModel(len(word2ix), embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    lr=LR,\n",
    "    scheduler_kwargs={'step_size': 10, 'gamma': 0.1},  # 每10个epoch学习率乘0.1, 避免后期学习率过大导致损失震荡\n",
    "    criterion=nn.CrossEntropyLoss(),  # 判据为交叉熵损失\n",
    "    tensorboard_path=''\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_words, ix2word, word2ix, max_gen_len, device):\n",
    "    results = []\n",
    "    input = torch.Tensor([word2ix['<START>']]).view(1, 1).long().to(device)  # 第一个字是<START>\n",
    "    hidden = None\n",
    "    model = model.to(device)\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_gen_len):  # 限制诗词最大长度\n",
    "            output, hidden = model(input, hidden)\n",
    "            if i < len(start_words):\n",
    "                # 在句首范围内持续将句首字作为输入\n",
    "                w = start_words[i]\n",
    "                ix = word2ix[w]\n",
    "            else:\n",
    "                ix = output.data[0].topk(1)[1][0].item()\n",
    "                w = ix2word[ix]\n",
    "                if w == '<EOP>':\n",
    "                    break\n",
    "            results.append(w)\n",
    "            input = input.data.new([ix]).view(1, 1)\n",
    "    return results\n",
    "\n",
    "def gen_acrostic(model, start_words_list, ix2word, word2ix, max_gen_len, device):\n",
    "    results = []\n",
    "    w = '<START>'\n",
    "    input = torch.Tensor([word2ix[w]]).view(1, 1).long().to(device)  # 第一个字是<START>\n",
    "    hidden = None\n",
    "    model = model.to(device)\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():\n",
    "        for start_words in start_words_list:\n",
    "            for i in range(max_gen_len):  # 限制诗词最大长度\n",
    "                output, hidden = model(input, hidden)\n",
    "                if i < len(start_words):\n",
    "                    # 在句首范围内持续将句首字作为输入\n",
    "                    w = start_words[i]\n",
    "                    ix = word2ix[w]\n",
    "                else:\n",
    "                    ix = output.data[0].topk(1)[1][0].item()\n",
    "                    w = ix2word[ix]\n",
    "                results.append(w)\n",
    "                input = input.data.new([ix]).view(1, 1)\n",
    "                if w == '<EOP>':\n",
    "                    results.pop()\n",
    "                if w in ['，', '。', '<EOP>']:\n",
    "                    break\n",
    "            if w == '<EOP>':\n",
    "                break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PoetryModel(len(word2ix), embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)\n",
    "model.load_state_dict(torch.load('model.pth'))  # 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一片鲎鱼壳，其中生翠波。买须能紫贝，用合对红螺。水岸吞空出，沙浑觅树行。何当得一息，借与此仙图。\n"
     ]
    }
   ],
   "source": [
    "print(''.join(i for i in generate(model, '一', ix2word, word2ix, 50, device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运背征蛮定不然，\n",
      "宵行永毕立搀环。\n",
      "可怜万物元不变，\n",
      "爱君堂上明玉声。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = ''.join(i for i in gen_acrostic(model, ['运', '宵', '可', '爱'], ix2word, word2ix, 50, device))\n",
    "result = result.replace('，', '，\\n')\n",
    "result = result.replace('。', '。\\n')\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
